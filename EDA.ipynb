{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pydub import AudioSegment\n",
    "np.random.seed(9) #For the Mighty Nein!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Broad Goals\n",
    "\n",
    "A Speech Emotion Recognition model allows a program to correctly identify the emotional state of a speaker.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Source\n",
    "\n",
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) includes 24 professional voice actors, 12 male, 12 female speaking the same two lines.  The full dataset includes both audio and visual files.  \n",
    "\n",
    "### Spoken Files\n",
    "| Actor |  Emotion | Intensity      | Statement | Repetition |\n",
    "| :---: |:--------:| :-------------:| :--------:| :--------: |\n",
    "| 01- 24| calm     | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| happy    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| sad      | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| angry    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| fearful  | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| surprise | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| disgust  | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| neutral  | normal         | 01, 02    |  01, 02    |\n",
    "\n",
    "Statement 01:  \"Kids are talking by the door\"\n",
    "\n",
    "Statement 02:  \"Dogs are sitting by the door\"\n",
    "\n",
    "Intensity 01:  \"Normal\"\n",
    "\n",
    "Intensity 02:  \"Strong\"\n",
    "\n",
    "### Song Files\n",
    "| Actor |  Emotion | Intensity      | Statement | Repetition |\n",
    "| :---: |:--------:| :-------------:| :--------:| :--------: |\n",
    "| 01- 24| calm     | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| happy    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| sad      | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| angry    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| fearful  | normal, strong | 01, 02    |  01, 02    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Full Dataset available at:\n",
    "https://smartlaboratory.org/ravdess/\n",
    "\n",
    "\n",
    "For the purposes of this project we will be examining the audio-only files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Problem Statement\n",
    "\n",
    "    How accurately can the model predict the emotion of the speaker?  Does the model lose or gain accuracy when the statement is spoken or sung?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2:  Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 define functions to parse sound files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma2=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma2))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01' : 'neutral',\n",
    "    '02' : 'calm',\n",
    "    '03' : 'happy',\n",
    "    '04' : 'sad',\n",
    "    '05' : 'angry',\n",
    "    '06' : 'fearful',\n",
    "    '07' : 'disgust',\n",
    "    '08' : 'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_channels = {\n",
    "    '01' : 'speech',\n",
    "    '02' : 'song'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_intensities = {\n",
    "    '01' : 'normal',\n",
    "    '02' : 'strong'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = {\n",
    "    '01' : 'Kids are talking by the door',\n",
    "    '02' : 'Dogs are sitting by the door'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size = 0.2):\n",
    "    feature_dicts = []\n",
    "    for file in glob.glob('./data/samples/*.wav'):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        channel = vocal_channels[file_name.split('-')[1]]\n",
    "        \n",
    "        feature = extract_feature(file, mfcc= True, chroma = True, mel = True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Exploratory Data Analysis\n",
    "\n",
    "## 3.1 Sampling Rates\n",
    "\n",
    "## 3.2 MFCC\n",
    "\n",
    "## 3.3 Chroma\n",
    "\n",
    "## 3.4 MEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit (conda)",
   "language": "python",
   "name": "python37764bitconda77dc9d652bc147c6a9d0ea8af6b5ba0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}