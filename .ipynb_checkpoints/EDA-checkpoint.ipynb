{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lb\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from librosa.display import specshow, waveplot\n",
    "from pydub import AudioSegment\n",
    "np.random.seed(9) #For the Mighty Nein!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Broad Goals\n",
    "\n",
    "A Speech Emotion Recognition model allows a program to correctly identify the emotional state of a speaker.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Source\n",
    "\n",
    "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) includes 24 professional voice actors, 12 male, 12 female speaking the same two lines.  The full dataset includes both audio and visual files.  \n",
    "\n",
    "### Spoken Files\n",
    "| Actor |  Emotion | Intensity      | Statement | Repetition |\n",
    "| :---: |:--------:| :-------------:| :--------:| :--------: |\n",
    "| 01- 24| calm     | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| happy    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| sad      | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| angry    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| fearful  | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| surprise | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| disgust  | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| neutral  | normal         | 01, 02    |  01, 02    |\n",
    "\n",
    "Statement 01:  \"Kids are talking by the door\"\n",
    "\n",
    "Statement 02:  \"Dogs are sitting by the door\"\n",
    "\n",
    "Intensity 01:  \"Normal\"\n",
    "\n",
    "Intensity 02:  \"Strong\"\n",
    "\n",
    "### Song Files\n",
    "| Actor |  Emotion | Intensity      | Statement | Repetition |\n",
    "| :---: |:--------:| :-------------:| :--------:| :--------: |\n",
    "| 01- 24| calm     | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| happy    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| sad      | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| angry    | normal, strong | 01, 02    |  01, 02    |\n",
    "| 01- 24| fearful  | normal, strong | 01, 02    |  01, 02    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Full Dataset available at:\n",
    "https://smartlaboratory.org/ravdess/\n",
    "\n",
    "\n",
    "For the purposes of this project we will be examining the audio-only files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Problem Statement\n",
    "\n",
    "    How accurately can the model predict the emotion of the speaker?  Does the model lose or gain accuracy when the statement is spoken or sung?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2:  Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 define functions to parse sound files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(lb.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(lb.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma2=np.mean(lb.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma2))\n",
    "        if mel:\n",
    "            mel=np.mean(lb.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    '01' : 'neutral',\n",
    "    '02' : 'calm',\n",
    "    '03' : 'happy',\n",
    "    '04' : 'sad',\n",
    "    '05' : 'angry',\n",
    "    '06' : 'fearful',\n",
    "    '07' : 'disgust',\n",
    "    '08' : 'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_channels = {\n",
    "    '01' : 'speech',\n",
    "    '02' : 'song'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_intensities = {\n",
    "    '01' : 'normal',\n",
    "    '02' : 'strong'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = {\n",
    "    '01' : 'Kids are talking by the door',\n",
    "    '02' : 'Dogs are sitting by the door'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    features = []\n",
    "    for file in glob.glob('./data/samples/*.wav'):\n",
    "        file_name = os.path.basename(file)\n",
    "        y, sr = lb.load(file)\n",
    "        feature_dict = {\n",
    "            'file_name' : file_name[:-4],\n",
    "            'channel' : vocal_channels[file_name.split('-')[1]],\n",
    "            'emotion' : emotions[file_name.split(\"-\")[2]],\n",
    "            'intensity' : emotional_intensities[file_name.split(\"-\")[3]],\n",
    "            'statement' : statements[file_name.split('-')[4]],\n",
    "            'repetition' : file_name.split('-')[5],\n",
    "            'actor' : file_name.split('-')[6][:-4],\n",
    "            'feature' : extract_feature(file, mfcc= True, chroma = True, mel = True),\n",
    "            'y': y,\n",
    "            'sr': sr }\n",
    "        features.append(feature_dict)\n",
    "#         print(F\"now loading {file_name}\")\n",
    "    return pd.DataFrame.from_dict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wp_targets(target_emotions, target_actor):\n",
    "#     #load files that contain target emotion\n",
    "#     speeches, songs = [], []\n",
    "    \n",
    "#     for file in glob.glob('./data/samples/*.wav'):\n",
    "#         file_name = os.path.basename(file)\n",
    "#         #filter out non-target emotions\n",
    "#         emotion = emotions[file_name.split('-')[2]]\n",
    "#         if emotion not in target_emotions:\n",
    "#             continue\n",
    "#         #filter out non-target actors\n",
    "#         actor = actors[file_name.split('-')[6][:-4]]\n",
    "#         if actor != target_actor:\n",
    "#             continue\n",
    "        \n",
    "#         y, sr = lb.load(file)\n",
    "        \n",
    "#         sound_dict = {\n",
    "#             'actor' : actor,\n",
    "#             'emotion': emotion,\n",
    "#             'y' : y\n",
    "            \n",
    "#         }\n",
    "        \n",
    "#         if file_name.split('-')[1] == '01';\n",
    "#             speeches.append(sound_dict)\n",
    "#         else:\n",
    "#             songs.append(sound_dict)\n",
    "            \n",
    "# #         return something?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "s_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrograms_and_waveform(y, sr):\n",
    "    fig, ax = plt.subplots(nrows = 3, ncols = 1, sharex = True, figsize=(12, 12))\n",
    "    D = lb.amplitude_to_db(np.abs(lb.stft(y)), ref=np.max)\n",
    "    img = lb.display.specshow(D, y_axis='linear', x_axis='time',\n",
    "                                sr=sr, ax=ax[0])\n",
    "    ax[0].set(title='Linear-frequency power spectrogram')\n",
    "    ax[0].label_outer()\n",
    "    hop_length = 1024\n",
    "    D = lb.amplitude_to_db(np.abs(lb.stft(y, hop_length=hop_length)),\n",
    "                                ref=np.max)\n",
    "    lb.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length,\n",
    "                            x_axis='time', ax=ax[1])\n",
    "    ax[1].set(title='Log-frequency power spectrogram')\n",
    "    ax[1].label_outer()\n",
    "    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "\n",
    "    lb.display.waveplot(y, sr)\n",
    "    ax[2].set(title='Waveplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrograms_and_waveform(s_df.loc[1, 'y'],  s_df.loc[1, 'sr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.display.waveplot(s_df.loc[18, 'y'], s_df.loc[18, 'sr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.display.waveplot(s_df.loc[569, 'y'], s_df.loc[569, 'sr']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, sharey = True, figsize=(12, 4))\n",
    "hop_length = 1024\n",
    "D = lb.amplitude_to_db(np.abs(lb.stft(s_df.loc[18, 'y'], hop_length=hop_length)),\n",
    "                                ref=np.max)\n",
    "img = lb.display.specshow(D, y_axis='log', sr=22050, hop_length=hop_length,\n",
    "                            x_axis='time', ax=ax[0])\n",
    "ax[0].set(title='Speech Log-frequency power spectrogram')\n",
    "ax[0].label_outer()\n",
    "hop_length = 1024\n",
    "D = lb.amplitude_to_db(np.abs(lb.stft(s_df.loc[569, 'y'], hop_length=hop_length)),\n",
    "                                ref=np.max)\n",
    "lb.display.specshow(D, y_axis='log', sr=22050, hop_length=hop_length,\n",
    "                            x_axis='time', ax=ax[1])\n",
    "ax[1].set(title='Song Log-frequency power spectrogram')\n",
    "ax[1].label_outer()\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_actor = s_df['actor'] == '02'\n",
    "m_statement = s_df['statement'] == 'Kids are talking by the door'\n",
    "tar_emotions = ['happy', 'sad', 'angry', 'fearful']\n",
    "m_emotion= s_df['emotion'].isin(tar_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_2 = s_df.loc[m_actor & m_statement & m_emotion, ['channel', 'y', 'emotion']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_2_songs = actor_2[actor_2['channel'] == 'song'].copy()\n",
    "actor_2_speeches = actor_2[actor_2['channel']== 'speech'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_2_songs.shape, actor_2_speeches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waves(sound_names,raw_sounds):\n",
    "    i = 1\n",
    "    fig = plt.figure(figsize=(25,60), dpi=900)\n",
    "    for n,f in zip(sound_names,raw_sounds):\n",
    "        plt.subplot(16,1,i)\n",
    "        lb.display.waveplot(np.array(f),sr=22050)\n",
    "        plt.title(n.title())\n",
    "        i += 1\n",
    "    plt.suptitle('Figure 1: Waveplot',x=0.5, y=0.915,fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waves(actor_2_songs['emotion'], actor_2_songs['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waves(actor_2_speeches['emotion'], actor_2_songs['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit (conda)",
   "language": "python",
   "name": "python37764bitconda77dc9d652bc147c6a9d0ea8af6b5ba0e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
